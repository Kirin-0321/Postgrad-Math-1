# 回归分析

## 简单线性回归模型

**简单线性回归**是研究一个因变量与一个自变量之间线性关系的统计方法。

### 模型的数学表达式

简单线性回归模型可以表示为：
$$Y = \beta_0 + \beta_1 X + \varepsilon$$

其中：
- $Y$ 是因变量(响应变量)
- $X$ 是自变量(解释变量)
- $\beta_0$ 是截距项，表示当 $X=0$ 时 $Y$ 的期望值
- $\beta_1$ 是斜率，表示 $X$ 每变化一个单位时 $Y$ 的平均变化量
- $\varepsilon$ 是随机误差项

### 基本假设

简单线性回归的基本假设包括：

1. **线性性**：自变量与因变量之间存在线性关系。
2. **独立性**：随机误差之间相互独立。
3. **同方差性**：随机误差具有恒定方差，即 $\text{Var}(\varepsilon) = \sigma^2$。
4. **正态性**：随机误差服从正态分布，即 $\varepsilon \sim N(0, \sigma^2)$。

注：有时也假设自变量 $X$ 是固定的，或者与随机误差 $\varepsilon$ 不相关。

### 参数估计

给定 $n$ 对观测值 $(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)$，可以使用**最小二乘法**估计参数 $\beta_0$ 和 $\beta_1$。

最小二乘法的目标是最小化残差平方和：
$$Q(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

求解方程组 $\frac{\partial Q}{\partial \beta_0} = 0$ 和 $\frac{\partial Q}{\partial \beta_1} = 0$，得到：

$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^{n} (x_i - \overline{x})^2} = \frac{S_{xy}}{S_{xx}}$$

$$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

其中，$\overline{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$，$\overline{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$。

### 显著性检验

#### 回归系数的显著性检验

对于假设 $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$，检验统计量为：

$$t = \frac{\hat{\beta}_1}{s_{\hat{\beta}_1}} \sim t(n-2)$$

其中，$s_{\hat{\beta}_1} = \frac{s_e}{\sqrt{S_{xx}}}$，$s_e = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n-2}}$。

当 $|t| > t_{\alpha/2}(n-2)$ 时，拒绝原假设，认为回归系数显著不为 0。

#### 回归方程的显著性检验

对于假设 $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$，可以通过方差分析进行检验：

$$F = \frac{SSR/1}{SSE/(n-2)} \sim F(1, n-2)$$

其中，$SSR = \sum_{i=1}^{n} (\hat{y}_i - \overline{y})^2$ 是回归平方和，$SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ 是残差平方和。

当 $F > F_{\alpha}(1, n-2)$ 时，拒绝原假设，认为回归方程显著。

### 拟合优度

**决定系数** $R^2$ 表示自变量能解释因变量变异的比例：

$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

其中，$SST = \sum_{i=1}^{n} (y_i - \overline{y})^2$ 是总平方和。

$R^2$ 的取值范围是 $[0, 1]$，越接近 1 表示拟合越好。

### 预测

对于给定的 $x_0$，因变量 $Y$ 的预测值为：

$$\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0$$

预测值的 $(1-\alpha)$ 置信区间为：

$$\hat{y}_0 \pm t_{\alpha/2}(n-2) \cdot s_e \cdot \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}}}$$

新观测值的 $(1-\alpha)$ 预测区间为：

$$\hat{y}_0 \pm t_{\alpha/2}(n-2) \cdot s_e \cdot \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}}}$$

## 多元线性回归模型

**多元线性回归**是研究一个因变量与多个自变量之间线性关系的统计方法。

### 模型的数学表达式

多元线性回归模型可以表示为：
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon$$

或用矩阵形式表示为：
$$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$

其中：
- $\mathbf{Y}$ 是 $n \times 1$ 维的因变量向量
- $\mathbf{X}$ 是 $n \times (p+1)$ 维的设计矩阵，其中第一列全为 1，对应截距项
- $\boldsymbol{\beta}$ 是 $(p+1) \times 1$ 维的参数向量
- $\boldsymbol{\varepsilon}$ 是 $n \times 1$ 维的随机误差向量

### 基本假设

多元线性回归的基本假设与简单线性回归类似：

1. **线性性**：因变量与自变量之间存在线性关系。
2. **独立性**：随机误差之间相互独立。
3. **同方差性**：随机误差具有恒定方差，即 $\text{Var}(\varepsilon_i) = \sigma^2$。
4. **正态性**：随机误差服从正态分布，即 $\varepsilon_i \sim N(0, \sigma^2)$。
5. **无多重共线性**：自变量之间不存在完全线性相关。

### 参数估计

使用最小二乘法估计参数 $\boldsymbol{\beta}$，目标是最小化残差平方和：
$$Q(\boldsymbol{\beta}) = \boldsymbol{\varepsilon}^T \boldsymbol{\varepsilon} = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})$$

求解方程：
$$\frac{\partial Q}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^T(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}) = \mathbf{0}$$

得到参数的最小二乘估计：
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$

### 显著性检验

#### 回归系数的显著性检验

对于假设 $H_0: \beta_j = 0$ vs $H_1: \beta_j \neq 0$，检验统计量为：

$$t_j = \frac{\hat{\beta}_j}{s_{\hat{\beta}_j}} \sim t(n-p-1)$$

其中，$s_{\hat{\beta}_j}$ 是 $\hat{\beta}_j$ 的标准误，等于 $s_e \cdot \sqrt{c_{jj}}$，$c_{jj}$ 是矩阵 $(\mathbf{X}^T\mathbf{X})^{-1}$ 的第 $j$ 个对角元素，$s_e = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n-p-1}}$。

当 $|t_j| > t_{\alpha/2}(n-p-1)$ 时，拒绝原假设，认为第 $j$ 个回归系数显著不为 0。

#### 回归方程的显著性检验

对于假设 $H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0$ vs $H_1:$ 至少存在一个 $\beta_j \neq 0$，可以通过方差分析进行检验：

$$F = \frac{SSR/p}{SSE/(n-p-1)} \sim F(p, n-p-1)$$

其中，$SSR = \sum_{i=1}^{n} (\hat{y}_i - \overline{y})^2$ 是回归平方和，$SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ 是残差平方和。

当 $F > F_{\alpha}(p, n-p-1)$ 时，拒绝原假设，认为回归方程显著。

### 拟合优度

**决定系数** $R^2$ 表示自变量能解释因变量变异的比例：

$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

其中，$SST = \sum_{i=1}^{n} (y_i - \overline{y})^2$ 是总平方和。

由于增加自变量会导致 $R^2$ 的增加，因此引入**调整决定系数**：

$$R_{adj}^2 = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$$

### 变量选择

常用的变量选择方法包括：

1. **前向选择法(Forward Selection)**：从一个空模型开始，每次添加一个最显著的变量，直到没有显著变量可添加。
2. **后向消除法(Backward Elimination)**：从包含所有变量的模型开始，每次删除一个最不显著的变量，直到所有变量都显著。
3. **逐步回归法(Stepwise Regression)**：结合前向选择和后向消除的方法，每次添加一个变量后，检查是否有变量可以删除。
4. **全子集回归(All Subsets Regression)**：考虑所有可能的变量组合，选择最优的子集。

选择模型的准则包括：
- **AIC (Akaike Information Criterion)**：$AIC = n \cdot \ln(SSE/n) + 2p$
- **BIC (Bayesian Information Criterion)**：$BIC = n \cdot \ln(SSE/n) + p \cdot \ln(n)$
- **Mallows' $C_p$**：$C_p = \frac{SSE}{\hat{\sigma}^2} + 2p - n$

通常选择 AIC、BIC 或 $C_p$ 最小的模型。

### 多重共线性

多重共线性指自变量之间存在高度相关性，会导致：
- 参数估计不稳定
- 标准误增大
- 显著性检验失效

判断多重共线性的方法：

1. **相关系数矩阵**：检查自变量之间的相关系数是否接近 1。
2. **方差膨胀因子(VIF)**：$VIF_j = \frac{1}{1-R_j^2}$，其中 $R_j^2$ 是第 $j$ 个自变量对其他自变量回归的决定系数。通常认为 $VIF > 10$ 表示存在严重的多重共线性。
3. **条件数**：设计矩阵 $\mathbf{X}$ 的特征值的最大值与最小值之比，若条件数大于 30，则认为存在多重共线性。

处理多重共线性的方法：

1. **删除变量**：删除高度相关的变量中的一部分。
2. **岭回归**：在最小二乘估计中添加一个惩罚项，得到 $\hat{\boldsymbol{\beta}}_{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{Y}$。
3. **主成分回归**：将自变量转换为互不相关的主成分，然后用主成分进行回归。
4. **LASSO 回归**：在最小二乘估计中添加 L1 范数惩罚项，可以实现变量选择。

## 模型诊断与改进

### 残差分析

残差是实际值与拟合值之间的差距：
$$e_i = y_i - \hat{y}_i$$

标准化残差为：
$$r_i = \frac{e_i}{s_e \cdot \sqrt{1 - h_{ii}}}$$

其中，$h_{ii}$ 是"帽子矩阵" $\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$ 的第 $i$ 个对角元素。

学生化残差为：
$$t_i = \frac{e_i}{s_{(i)} \cdot \sqrt{1 - h_{ii}}}$$

其中，$s_{(i)}$ 是不包括第 $i$ 个观测值时的残差标准差估计。

可以通过以下方法对残差进行分析：

1. **残差图**：绘制残差对自变量或拟合值的散点图，检查线性性和同方差性假设。
2. **正态 Q-Q 图**：检查残差的正态性假设。
3. **残差自相关图**：检查残差的独立性假设。

### 影响分析

影响分析用于识别对回归结果有较大影响的观测值。常用的影响度量包括：

1. **杠杆值(Leverage)**：即帽子矩阵的对角元素 $h_{ii}$，表示第 $i$ 个观测值对应的拟合值受该观测值的影响程度。通常认为 $h_{ii} > 2(p+1)/n$ 的观测值为高杠杆点。
2. **Cook 距离**：测量删除第 $i$ 个观测值对所有拟合值的影响：
   $$D_i = \frac{r_i^2}{p+1} \cdot \frac{h_{ii}}{1-h_{ii}}$$
   通常认为 $D_i > 1$ 的观测值为有影响的观测值。
3. **DFFITS**：测量删除第 $i$ 个观测值对第 $i$ 个拟合值的影响：
   $$DFFITS_i = r_i \cdot \sqrt{\frac{h_{ii}}{1-h_{ii}}}$$
4. **DFBETAS**：测量删除第 $i$ 个观测值对回归系数的影响：
   $$DFBETAS_{ij} = \frac{\hat{\beta}_j - \hat{\beta}_{j(i)}}{s_{(i)} \cdot \sqrt{c_{jj}}}$$

### 变换

当回归模型的假设不满足时，可以通过变换来改进模型：

1. **因变量的变换**：如取对数、平方根等，主要用于解决非线性和异方差问题。
2. **自变量的变换**：如取对数、平方等，主要用于解决非线性问题。
3. **Box-Cox 变换**：一种常用的幂变换，对因变量 $Y$ 进行变换：
   $$Y^{(\lambda)} = \begin{cases}
   \frac{Y^{\lambda} - 1}{\lambda}, & \lambda \neq 0 \\
   \ln(Y), & \lambda = 0
   \end{cases}$$
   通过最大似然法选择最优的 $\lambda$ 值。

## 非线性回归

当因变量与自变量之间的关系不能用线性模型描述时，可以使用非线性回归模型。

### 多项式回归

多项式回归是一种简单的非线性回归，模型为：
$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \beta_p X^p + \varepsilon$$

这可以通过引入新变量 $X^2, X^3, \cdots, X^p$ 转化为线性回归模型。

### 对数线性模型

对数线性模型包括以下几种常见形式：

1. **对数-对数模型**：$\ln(Y) = \beta_0 + \beta_1 \ln(X) + \varepsilon$
   
   这种模型中，$\beta_1$ 表示弹性，即 $X$ 增加 1%，$Y$ 平均增加 $\beta_1$%。

2. **半对数模型**：$\ln(Y) = \beta_0 + \beta_1 X + \varepsilon$ 或 $Y = \beta_0 + \beta_1 \ln(X) + \varepsilon$

   在第一种情况下，$X$ 增加一个单位，$Y$ 平均增加 $100 \cdot \beta_1$%。

### 非线性最小二乘法

对于一般形式的非线性模型 $Y = f(X, \boldsymbol{\beta}) + \varepsilon$，可以使用非线性最小二乘法估计参数：

$$\min_{\boldsymbol{\beta}} \sum_{i=1}^{n} [y_i - f(x_i, \boldsymbol{\beta})]^2$$

这通常需要使用迭代算法求解，如 Gauss-Newton 法或 Levenberg-Marquardt 法。

## 广义线性模型

**广义线性模型**(Generalized Linear Models, GLM)是线性回归模型的推广，适用于因变量不服从正态分布的情况。

### 模型组成

广义线性模型由三个部分组成：

1. **随机分量**：因变量 $Y$ 独立地服从指数族分布，如正态分布、二项分布、泊松分布等。
2. **系统分量**：线性预测量 $\eta = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$。
3. **连接函数**：连接函数 $g$ 满足 $g(\mu) = \eta$，其中 $\mu = E(Y)$。

### 常见的广义线性模型

1. **线性回归**：
   - 分布：正态分布 $Y \sim N(\mu, \sigma^2)$
   - 连接函数：恒等函数 $g(\mu) = \mu$

2. **逻辑回归**：
   - 分布：二项分布 $Y \sim Bin(n, p)$
   - 连接函数：Logit 函数 $g(p) = \ln\left(\frac{p}{1-p}\right)$

3. **泊松回归**：
   - 分布：泊松分布 $Y \sim Poisson(\lambda)$
   - 连接函数：对数函数 $g(\lambda) = \ln(\lambda)$

### 参数估计

广义线性模型的参数通常使用最大似然法估计，需要使用迭代加权最小二乘法(IWLS)求解。

### 模型评估

广义线性模型的评估指标包括：

1. **偏差(Deviance)**：模型对数似然与饱和模型对数似然之差的 -2 倍，衡量模型拟合优度。
2. **AIC (Akaike Information Criterion)**：$AIC = -2 \ln(L) + 2p$，其中 $L$ 是似然函数，$p$ 是参数个数。
3. **BIC (Bayesian Information Criterion)**：$BIC = -2 \ln(L) + p \cdot \ln(n)$。

模型诊断可以通过残差分析进行，如使用 Pearson 残差或偏差残差。

## 时间序列回归

时间序列回归是处理随时间变化的数据的一类特殊回归模型。

### 自回归模型

**自回归模型**(Autoregressive Model, AR)表示当前值依赖于过去的值：

$$Y_t = \phi_0 + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_p Y_{t-p} + \varepsilon_t$$

这是一个 AR(p) 模型，其中 $p$ 是阶数。

### 移动平均模型

**移动平均模型**(Moving Average Model, MA)表示当前值依赖于过去的白噪声：

$$Y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q}$$

这是一个 MA(q) 模型，其中 $q$ 是阶数。

### ARIMA 模型

**自回归综合移动平均模型**(Autoregressive Integrated Moving Average Model, ARIMA)是结合自回归、差分和移动平均的模型，表示为 ARIMA(p, d, q)，其中：
- $p$ 是自回归项数
- $d$ 是差分次数
- $q$ 是移动平均项数

ARIMA 模型的一般形式为：

$$(1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p)(1 - B)^d Y_t = \mu + (1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q)\varepsilon_t$$

其中，$B$ 是后移算子，即 $BY_t = Y_{t-1}$。

### 季节性 ARIMA 模型

季节性 ARIMA 模型，表示为 SARIMA(p, d, q)(P, D, Q)s，用于处理具有季节性的时间序列。

## 面板数据回归

面板数据是同时包含横截面和时间序列的数据。面板数据回归模型包括：

### 固定效应模型

固定效应模型假设个体间存在不可观测的固定差异：

$$Y_{it} = \alpha_i + \beta_1 X_{1it} + \beta_2 X_{2it} + \cdots + \beta_p X_{pit} + \varepsilon_{it}$$

其中，$\alpha_i$ 是第 $i$ 个个体的固定效应。

### 随机效应模型

随机效应模型假设个体效应是随机的：

$$Y_{it} = \alpha + \beta_1 X_{1it} + \beta_2 X_{2it} + \cdots + \beta_p X_{pit} + u_i + \varepsilon_{it}$$

其中，$u_i$ 是随机效应，$u_i \sim N(0, \sigma_u^2)$。

### Hausman 检验

Hausman 检验用于判断应该使用固定效应模型还是随机效应模型：

$$H = (\hat{\boldsymbol{\beta}}_{FE} - \hat{\boldsymbol{\beta}}_{RE})^T [\text{Var}(\hat{\boldsymbol{\beta}}_{FE}) - \text{Var}(\hat{\boldsymbol{\beta}}_{RE})]^{-1} (\hat{\boldsymbol{\beta}}_{FE} - \hat{\boldsymbol{\beta}}_{RE})$$

在原假设（随机效应模型适合）下，$H \sim \chi^2(p)$。如果 $H > \chi^2_{\alpha}(p)$，则拒绝原假设，认为应该使用固定效应模型。